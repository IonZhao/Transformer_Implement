{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Model Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # self.device = 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    # def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "    #     \"\"\"\n",
    "    #     train_iters = how many training iterations\n",
    "    #     eval_iters = how many batches to evaluate to get average performance\n",
    "    #     \"\"\"\n",
    "    #     optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    #     for iteration in range(train_iters):\n",
    "    #         if iteration % eval_iters == 0:\n",
    "    #             avg_loss = self.eval_loss(eval_iters)\n",
    "    #             print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "    #         inputs, targets = self.get_batch(split='train')\n",
    "    #         _, ce_loss = self(inputs, targets)\n",
    "    #         optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "    #         ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "    #         optimizer.step()  # update network parameters w.r.t the loss\n",
    "    #     # torch.save(self, 'sa_pos_')\n",
    "    \n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_metrics = self.eval_loss(eval_iters)\n",
    "                print(\n",
    "                    f\"iter {iteration}: \"\n",
    "                    f\"train loss {avg_metrics['train']['loss']:.4f}, \"\n",
    "                    f\"train perplexity {avg_metrics['train']['perplexity']:.4f}, \"\n",
    "                    f\"val loss {avg_metrics['eval']['loss']:.4f}, \"\n",
    "                    f\"val perplexity {avg_metrics['eval']['perplexity']:.4f}\"\n",
    "                )\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    # @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    # def eval_loss(self, eval_iters):\n",
    "    #     perf = {}\n",
    "    #     # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "    #     self.eval()\n",
    "    #     for split in ['train', 'eval']:\n",
    "    #         losses = torch.zeros(eval_iters)\n",
    "    #         for k in range(eval_iters):\n",
    "    #             tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "    #             _, ce_loss = self(tokens, targets)  # forward pass\n",
    "    #             losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "    #         perf[split] = losses.mean()\n",
    "    #     self.train()  # turn-on training mode-\n",
    "    #     return perf\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)\n",
    "                _, ce_loss = self(tokens, targets)\n",
    "                losses[k] = ce_loss.item()\n",
    "            avg_loss = losses.mean()\n",
    "            perplexity = torch.exp(avg_loss)\n",
    "            perf[split] = {'loss': avg_loss.item(), 'perplexity': perplexity.item()}\n",
    "        self.train()  # Set the model back to training mode\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        # two dictionaries to map characters to integers and vice-versa\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        # encoder and decoder functions\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/WarrenBuffet.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3  Model Tranning and content generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 1115739\n",
      "iter 0: train loss 5.8728, train perplexity 355.2481, val loss 5.8665, val perplexity 353.0027\n",
      "iter 1000: train loss 1.5300, train perplexity 4.6184, val loss 1.6477, val perplexity 5.1951\n",
      "iter 2000: train loss 1.3305, train perplexity 3.7829, val loss 1.5156, val perplexity 4.5521\n",
      "iter 3000: train loss 1.2082, train perplexity 3.3475, val loss 1.4884, val perplexity 4.4299\n",
      "\n",
      "\n",
      "At Berkshire. \n",
      "\n",
      "The annual investment neve. But housing propertise analysping table, \n",
      "the focuse of our expense condition distributions requirerther to Berkshire. Charlie and I treat the mortgages fell of GEICOs for evaluating were did! Todd company has outhing repurchases are are given your stock insurers \n",
      "are an activity to one have their earnings exbaoviar. These \n",
      "ehred; of I first of how many govement by five investments. \n",
      "\n",
      "We rece. Even after behind them amountearful housing stracts well. Here are sometimes of the inwest hands, account might run by Marmomeones for Crmbitably-os net incereased book value of medical sides with all \n",
      "conform as a locked.) \n",
      "\n",
      "The first of our five years appeared. \n",
      "\n",
      "Regulated, the United amount important of our managers were can site is out their acress. We tear earnings from Tony \n",
      "Tad anment of \n",
      "its 30% of a modest \n",
      "profit in 2010. \n",
      "At Berkshire, we get by 2006 American have its donesquality because of these \n",
      "formuce. \n",
      "\n",
      "Charlie annually, Moldy accompou\n"
     ]
    }
   ],
   "source": [
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model.prep(text)\n",
    "model = model.to(model.device)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Analyzing\n",
    "\n",
    "## 4.1 Analyzing the process\n",
    "\n",
    "At the start of training, both the training and validation perplexities are extremely high (around 355). This indicates that the model is initially performing poorly, as it has not yet learned from the data and is essentially making random predictions.\n",
    "Improvement Over Iterations:\n",
    "\n",
    "By iteration 1000, the training perplexity drops dramatically to approximately 4.62, and the validation perplexity to approximately 5.20.\n",
    "At iteration 2000, perplexities further decrease to 3.78 (train) and 4.55 (val).\n",
    "By iteration 3000, perplexities are 3.35 (train) and 4.43 (val).\n",
    "\n",
    "The training perplexity is consistently lower than the validation perplexity, which is expected because the model is directly trained on the training data.\n",
    "The relatively small gap between training and validation perplexities suggests that the model is not overfitting significantly and is generalizing well to unseen data.\n",
    "\n",
    "The model's performance improves significantly over the training iterations, as evidenced by the decreasing perplexity. The low perplexity values indicate that the model has effectively learned to predict the next character in the text, capturing the underlying structure and patterns of the language in the training data.\n",
    "\n",
    "## 4.2 Analyzing the output\n",
    "\n",
    "The text frequently mentions \"Berkshire,\" \"Charlie and I,\" \"GEICO,\" \"investments,\" \"managers,\" and \"earnings,\" which are terms associated with financial reports and shareholder letters, particularly those from Berkshire Hathaway.\n",
    "\n",
    "Sentences like \"The first of our five years appeared.\" and \"At Berkshire, we get by 2006 American have its...\" show some level of coherence. The model captures the style of formal reports, using phrases like \"annual investment,\" \"housing properties,\" \"book value,\" and \"profit in 2010.\"\n",
    "\n",
    "The most impressive aspect of the generated text is its ability to stay on topic and use domain-specific vocabulary relevant to financial reports. The high-impact design choices, particularly the use of positional embeddings, multi-head self-attention, and multiple Transformer layers with sufficient embedding dimensions, have enabled the model to capture complex language patterns.\n",
    "\n",
    "While the model exhibits limitations, such as generating nonsensical words and occasional grammatical errors, these issues are common in character-level models with limited training data. Increasing the dataset size, transitioning to word-level modeling, or further tuning the model's hyperparameters could help improve the coherence and accuracy of the generated text.\n",
    "\n",
    "By analyzing perplexity and the generated text, it's clear that the model has learned significant aspects of the language patterns in the training data. The design choices made have a substantial impact on the model's ability to generate text that is thematically consistent and occasionally coherent, demonstrating the effectiveness of Transformer architectures in language modeling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
